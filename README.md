# EMG Signal Classification using CNN

This repository contains the implementation of a Convolutional Neural Network (CNN) model for classifying hand gestures based on Electromyography (EMG) signals. The work uses a publicly available dataset, processing raw EMG signals into structured data, training various CNN architectures, and evaluating model performance. This project was developed as part of the Google AI for Impact Hackathon.

## Dataset

The dataset used in this project is the **EMG Data for Gestures** dataset, available at [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/481/emg+data+for+gestures). The dataset contains:

- **Source:** EMG signals measured using a MYO Thalmic bracelet equipped with eight sensors evenly spaced around the forearm.
- **Subjects:** 36 different individuals performing 6-7 static hand gestures. For this project, only the first six gestures are used due to inconsistencies in gesture seven across subjects.
- **Signal Recording:** Signals are sampled every millisecond and transmitted via Bluetooth to a PC for storage.
- **Gesture Durations:** Each gesture is held for 3 seconds, with 3-second pauses between gestures.
- **Number of Instances:** Approximately 40,000-50,000 recordings per column, with 30,000 guaranteed.
- **Windowing:** Windows of **N=800 consecutive instances** of 8 signals are used to represent a specific gesture.

The dataset is split by subjects to evaluate the modelâ€™s generalization to unseen individuals:

- **Training Set:** 28 subjects
- **Validation Set:** 5 subjects
- **Testing Set:** 3 subjects

## Repository Structure

- **`utils.py`**: Contains utility functions for preprocessing and handling data.

  - Data division by subject to ensure balanced splits.
  - Sliding window generation for creating training samples.
  - Normalization using MinMaxScaler for consistent input scaling.
  - Data augmentation techniques like adding Gaussian noise.
  - Functions for training the model and evaluating predictions.

- **`models.py`**: Defines three versions of CNN architectures:

  - **`EMG2DClassifier_V0`**: Basic architecture with low dropout.
  - **`EMG2DClassifier_V1`**: Enhanced architecture with larger convolution kernels.
  - **`EMG2DClassifier_V2`**: Advanced architecture with higher dropout for regularization.

  Each model uses:

  - 2D convolutional layers for feature extraction.
  - Batch normalization and ReLU activation for stability and non-linearity.
  - Dropout layers to prevent overfitting.
  - Fully connected layers to produce gesture predictions.

- **`EMG_Classification_CNN.ipynb`**: Jupyter Notebook containing the training and evaluation pipeline.

## Methodology

1. **Preprocessing**

   - EMG signals are normalized using MinMaxScaler.
   - Sliding windows of 800 ms are created, corresponding to gestures.
   - Augmented datasets are generated by adding Gaussian noise.

2. **Model Training**

   - Each CNN model is trained using the processed data.
   - Early stopping is applied based on validation accuracy to avoid overfitting.
   - The best-performing model is saved for evaluation.

3. **Evaluation**
   - Predictions are generated for the test set.
   - Performance metrics are calculated and results are stored in a structured format for analysis.

## Results

The model demonstrates strong performance in classifying gestures from unseen subjects, showcasing its generalization capabilities. Specific performance metrics and results can be found in the notebook.

## How to Use

1. Clone the repository:

   ```bash
   git clone https://github.com/your-username/EMG-Signal-Classification-using-CNN.git
   cd EMG-Signal-Classification-using-CNN
   ```

2. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

3. Run the  Python Script:

   ```bash
   python EMG_Classification_CNN.py
   ```

4. Train the model and evaluate the results.

## Acknowledgments

This project uses the EMG Data for Gestures dataset provided by the UCI Machine Learning Repository. Special thanks to the organizers of the Google AI for Impact Hackathon for providing the opportunity to work on this challenge.
